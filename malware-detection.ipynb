{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7959081,"sourceType":"datasetVersion","datasetId":4681815}],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"# !pip install seaborn\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn\nimport tensorflow as tf\nimport polars as pl\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, normalize\nfrom sklearn.linear_model import LogisticRegression\nfrom lightgbm import LGBMClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report, auc\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neural_network import MLPClassifier\nimport xgboost as xgb\nimport keras\nimport tensorflow as tf\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom time import time\nimport numpy as np\nfrom sklearn.svm import LinearSVC, SVC\nfrom tqdm import tqdm\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import cross_val_score","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:28:50.081323Z","iopub.execute_input":"2024-06-13T10:28:50.081746Z","iopub.status.idle":"2024-06-13T10:28:50.091048Z","shell.execute_reply.started":"2024-06-13T10:28:50.081706Z","shell.execute_reply":"2024-06-13T10:28:50.089870Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/cleaned-malware-detection/Cleaned_malware.csv\", low_memory=False)\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:14:16.082449Z","iopub.execute_input":"2024-06-13T10:14:16.083116Z","iopub.status.idle":"2024-06-13T10:15:03.439328Z","shell.execute_reply.started":"2024-06-13T10:14:16.083081Z","shell.execute_reply":"2024-06-13T10:15:03.438044Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"   Unnamed: 0                 MachineIdentifier   ProductName EngineVersion  \\\n0           5  000016191b897145d069102325cab760  win8defender   1.1.15100.1   \n1           7  000019515bc8f95851aff6de873405e8  win8defender   1.1.15100.1   \n2           8  00001a027a0ab970c408182df8484fce  win8defender   1.1.15200.1   \n3          15  000027c68b89acb49d4017763b043449  win8defender   1.1.15200.1   \n4          24  000038f24a1ee98931456b3e49f7934b  win8defender   1.1.15200.1   \n\n        AppVersion  AvSigVersion  IsBeta  RtpStateBitfield  IsSxsPassiveMode  \\\n0  4.18.1807.18075  1.273.1094.0       0               7.0                 0   \n1  4.18.1807.18075  1.273.1393.0       0               7.0                 0   \n2  4.18.1807.18075   1.275.988.0       0               7.0                 0   \n3  4.18.1807.18075   1.275.130.0       0               7.0                 0   \n4  4.18.1806.18062   1.275.879.0       0               7.0                 0   \n\n   AVProductStatesIdentifier  ...  Census_FirmwareVersionIdentifier  \\\n0                    53447.0  ...                           51039.0   \n1                    53447.0  ...                           63122.0   \n2                    53447.0  ...                           15510.0   \n3                    47238.0  ...                           19951.0   \n4                    53447.0  ...                           51032.0   \n\n   Census_IsSecureBootEnabled  Census_IsWIMBootEnabled  \\\n0                           0                      0.0   \n1                           0                      0.0   \n2                           0                      0.0   \n3                           1                      0.0   \n4                           0                      0.0   \n\n   Census_IsVirtualDevice  Census_IsTouchEnabled  Census_IsPenCapable  \\\n0                     0.0                      0                    0   \n1                     0.0                      0                    0   \n2                     0.0                      0                    0   \n3                     0.0                      0                    0   \n4                     0.0                      0                    0   \n\n   Census_IsAlwaysOnAlwaysConnectedCapable  Wdft_IsGamer  \\\n0                                      0.0           0.0   \n1                                      0.0           0.0   \n2                                      0.0           0.0   \n3                                      0.0           0.0   \n4                                      0.0           0.0   \n\n  Wdft_RegionIdentifier HasDetections  \n0                  15.0             1  \n1                  15.0             0  \n2                  15.0             0  \n3                   1.0             1  \n4                   7.0             0  \n\n[5 rows x 79 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>MachineIdentifier</th>\n      <th>ProductName</th>\n      <th>EngineVersion</th>\n      <th>AppVersion</th>\n      <th>AvSigVersion</th>\n      <th>IsBeta</th>\n      <th>RtpStateBitfield</th>\n      <th>IsSxsPassiveMode</th>\n      <th>AVProductStatesIdentifier</th>\n      <th>...</th>\n      <th>Census_FirmwareVersionIdentifier</th>\n      <th>Census_IsSecureBootEnabled</th>\n      <th>Census_IsWIMBootEnabled</th>\n      <th>Census_IsVirtualDevice</th>\n      <th>Census_IsTouchEnabled</th>\n      <th>Census_IsPenCapable</th>\n      <th>Census_IsAlwaysOnAlwaysConnectedCapable</th>\n      <th>Wdft_IsGamer</th>\n      <th>Wdft_RegionIdentifier</th>\n      <th>HasDetections</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5</td>\n      <td>000016191b897145d069102325cab760</td>\n      <td>win8defender</td>\n      <td>1.1.15100.1</td>\n      <td>4.18.1807.18075</td>\n      <td>1.273.1094.0</td>\n      <td>0</td>\n      <td>7.0</td>\n      <td>0</td>\n      <td>53447.0</td>\n      <td>...</td>\n      <td>51039.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>15.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7</td>\n      <td>000019515bc8f95851aff6de873405e8</td>\n      <td>win8defender</td>\n      <td>1.1.15100.1</td>\n      <td>4.18.1807.18075</td>\n      <td>1.273.1393.0</td>\n      <td>0</td>\n      <td>7.0</td>\n      <td>0</td>\n      <td>53447.0</td>\n      <td>...</td>\n      <td>63122.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>15.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>8</td>\n      <td>00001a027a0ab970c408182df8484fce</td>\n      <td>win8defender</td>\n      <td>1.1.15200.1</td>\n      <td>4.18.1807.18075</td>\n      <td>1.275.988.0</td>\n      <td>0</td>\n      <td>7.0</td>\n      <td>0</td>\n      <td>53447.0</td>\n      <td>...</td>\n      <td>15510.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>15.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>15</td>\n      <td>000027c68b89acb49d4017763b043449</td>\n      <td>win8defender</td>\n      <td>1.1.15200.1</td>\n      <td>4.18.1807.18075</td>\n      <td>1.275.130.0</td>\n      <td>0</td>\n      <td>7.0</td>\n      <td>0</td>\n      <td>47238.0</td>\n      <td>...</td>\n      <td>19951.0</td>\n      <td>1</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>24</td>\n      <td>000038f24a1ee98931456b3e49f7934b</td>\n      <td>win8defender</td>\n      <td>1.1.15200.1</td>\n      <td>4.18.1806.18062</td>\n      <td>1.275.879.0</td>\n      <td>0</td>\n      <td>7.0</td>\n      <td>0</td>\n      <td>53447.0</td>\n      <td>...</td>\n      <td>51032.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>7.0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 79 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"data.drop(\"Unnamed: 0\", axis = 1, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:15:03.440818Z","iopub.execute_input":"2024-06-13T10:15:03.441257Z","iopub.status.idle":"2024-06-13T10:15:04.291750Z","shell.execute_reply.started":"2024-06-13T10:15:03.441218Z","shell.execute_reply":"2024-06-13T10:15:04.290397Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"data.info()","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:15:04.293827Z","iopub.execute_input":"2024-06-13T10:15:04.294170Z","iopub.status.idle":"2024-06-13T10:15:04.322763Z","shell.execute_reply.started":"2024-06-13T10:15:04.294143Z","shell.execute_reply":"2024-06-13T10:15:04.321254Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1823550 entries, 0 to 1823549\nData columns (total 78 columns):\n #   Column                                             Dtype  \n---  ------                                             -----  \n 0   MachineIdentifier                                  object \n 1   ProductName                                        object \n 2   EngineVersion                                      object \n 3   AppVersion                                         object \n 4   AvSigVersion                                       object \n 5   IsBeta                                             int64  \n 6   RtpStateBitfield                                   float64\n 7   IsSxsPassiveMode                                   int64  \n 8   AVProductStatesIdentifier                          float64\n 9   AVProductsInstalled                                float64\n 10  AVProductsEnabled                                  float64\n 11  HasTpm                                             int64  \n 12  CountryIdentifier                                  int64  \n 13  CityIdentifier                                     float64\n 14  OrganizationIdentifier                             float64\n 15  GeoNameIdentifier                                  float64\n 16  LocaleEnglishNameIdentifier                        int64  \n 17  Platform                                           object \n 18  Processor                                          object \n 19  OsVer                                              object \n 20  OsBuild                                            int64  \n 21  OsSuite                                            int64  \n 22  OsPlatformSubRelease                               object \n 23  OsBuildLab                                         object \n 24  SkuEdition                                         object \n 25  IsProtected                                        float64\n 26  AutoSampleOptIn                                    int64  \n 27  SMode                                              float64\n 28  IeVerIdentifier                                    float64\n 29  SmartScreen                                        object \n 30  Firewall                                           float64\n 31  UacLuaenable                                       float64\n 32  Census_MDC2FormFactor                              object \n 33  Census_DeviceFamily                                object \n 34  Census_OEMNameIdentifier                           float64\n 35  Census_OEMModelIdentifier                          float64\n 36  Census_ProcessorCoreCount                          float64\n 37  Census_ProcessorManufacturerIdentifier             float64\n 38  Census_ProcessorModelIdentifier                    float64\n 39  Census_PrimaryDiskTotalCapacity                    float64\n 40  Census_PrimaryDiskTypeName                         object \n 41  Census_SystemVolumeTotalCapacity                   float64\n 42  Census_HasOpticalDiskDrive                         int64  \n 43  Census_TotalPhysicalRAM                            float64\n 44  Census_ChassisTypeName                             object \n 45  Census_InternalPrimaryDiagonalDisplaySizeInInches  float64\n 46  Census_InternalPrimaryDisplayResolutionHorizontal  float64\n 47  Census_InternalPrimaryDisplayResolutionVertical    float64\n 48  Census_PowerPlatformRoleName                       object \n 49  Census_InternalBatteryNumberOfCharges              float64\n 50  Census_OSVersion                                   object \n 51  Census_OSArchitecture                              object \n 52  Census_OSBranch                                    object \n 53  Census_OSBuildNumber                               int64  \n 54  Census_OSBuildRevision                             int64  \n 55  Census_OSEdition                                   object \n 56  Census_OSSkuName                                   object \n 57  Census_OSInstallTypeName                           object \n 58  Census_OSInstallLanguageIdentifier                 float64\n 59  Census_OSUILocaleIdentifier                        int64  \n 60  Census_OSWUAutoUpdateOptionsName                   object \n 61  Census_IsPortableOperatingSystem                   int64  \n 62  Census_GenuineStateName                            object \n 63  Census_ActivationChannel                           object \n 64  Census_IsFlightsDisabled                           float64\n 65  Census_FlightRing                                  object \n 66  Census_ThresholdOptIn                              float64\n 67  Census_FirmwareManufacturerIdentifier              float64\n 68  Census_FirmwareVersionIdentifier                   float64\n 69  Census_IsSecureBootEnabled                         int64  \n 70  Census_IsWIMBootEnabled                            float64\n 71  Census_IsVirtualDevice                             float64\n 72  Census_IsTouchEnabled                              int64  \n 73  Census_IsPenCapable                                int64  \n 74  Census_IsAlwaysOnAlwaysConnectedCapable            float64\n 75  Wdft_IsGamer                                       float64\n 76  Wdft_RegionIdentifier                              float64\n 77  HasDetections                                      int64  \ndtypes: float64(34), int64(17), object(27)\nmemory usage: 1.1+ GB\n","output_type":"stream"}]},{"cell_type":"code","source":"data.describe()","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:15:04.324402Z","iopub.execute_input":"2024-06-13T10:15:04.324840Z","iopub.status.idle":"2024-06-13T10:15:07.622248Z","shell.execute_reply.started":"2024-06-13T10:15:04.324801Z","shell.execute_reply":"2024-06-13T10:15:07.621038Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"             IsBeta  RtpStateBitfield  IsSxsPassiveMode  \\\ncount  1.823550e+06      1.823550e+06      1.823550e+06   \nmean   2.193524e-06      6.847975e+00      1.676017e-02   \nstd    1.481054e-03      1.006668e+00      1.283716e-01   \nmin    0.000000e+00      0.000000e+00      0.000000e+00   \n25%    0.000000e+00      7.000000e+00      0.000000e+00   \n50%    0.000000e+00      7.000000e+00      0.000000e+00   \n75%    0.000000e+00      7.000000e+00      0.000000e+00   \nmax    1.000000e+00      8.000000e+00      1.000000e+00   \n\n       AVProductStatesIdentifier  AVProductsInstalled  AVProductsEnabled  \\\ncount               1.823550e+06         1.823550e+06       1.823550e+06   \nmean                4.701612e+04         1.383484e+00       1.026553e+00   \nstd                 1.470503e+04         5.430370e-01       1.835109e-01   \nmin                 7.000000e+00         1.000000e+00       0.000000e+00   \n25%                 4.681000e+04         1.000000e+00       1.000000e+00   \n50%                 5.344700e+04         1.000000e+00       1.000000e+00   \n75%                 5.344700e+04         2.000000e+00       1.000000e+00   \nmax                 7.049800e+04         6.000000e+00       5.000000e+00   \n\n             HasTpm  CountryIdentifier  CityIdentifier  \\\ncount  1.823550e+06       1.823550e+06    1.823550e+06   \nmean   9.989625e-01       1.069236e+02    8.060299e+04   \nstd    3.219411e-02       6.277194e+01    4.913968e+04   \nmin    0.000000e+00       1.000000e+00    7.000000e+00   \n25%    1.000000e+00       5.100000e+01    3.480900e+04   \n50%    1.000000e+00       9.400000e+01    8.237300e+04   \n75%    1.000000e+00       1.600000e+02    1.241500e+05   \nmax    1.000000e+00       2.220000e+02    1.679620e+05   \n\n       OrganizationIdentifier  ...  Census_FirmwareVersionIdentifier  \\\ncount            1.823550e+06  ...                      1.823550e+06   \nmean             2.484157e+01  ...                      3.413802e+04   \nstd              5.195289e+00  ...                      2.095789e+04   \nmin              1.000000e+00  ...                      8.700000e+01   \n25%              1.800000e+01  ...                      1.704100e+04   \n50%              2.700000e+01  ...                      3.307000e+04   \n75%              2.700000e+01  ...                      5.289700e+04   \nmax              5.200000e+01  ...                      7.209600e+04   \n\n       Census_IsSecureBootEnabled  Census_IsWIMBootEnabled  \\\ncount                1.823550e+06                1823550.0   \nmean                 5.144592e-01                      0.0   \nstd                  4.997910e-01                      0.0   \nmin                  0.000000e+00                      0.0   \n25%                  0.000000e+00                      0.0   \n50%                  1.000000e+00                      0.0   \n75%                  1.000000e+00                      0.0   \nmax                  1.000000e+00                      0.0   \n\n       Census_IsVirtualDevice  Census_IsTouchEnabled  Census_IsPenCapable  \\\ncount            1.823550e+06           1.823550e+06         1.823550e+06   \nmean             1.355598e-03           1.460031e-01         3.628362e-02   \nstd              3.679349e-02           3.531094e-01         1.869950e-01   \nmin              0.000000e+00           0.000000e+00         0.000000e+00   \n25%              0.000000e+00           0.000000e+00         0.000000e+00   \n50%              0.000000e+00           0.000000e+00         0.000000e+00   \n75%              0.000000e+00           0.000000e+00         0.000000e+00   \nmax              1.000000e+00           1.000000e+00         1.000000e+00   \n\n       Census_IsAlwaysOnAlwaysConnectedCapable  Wdft_IsGamer  \\\ncount                             1.823550e+06  1.823550e+06   \nmean                              6.411066e-02  2.223334e-01   \nstd                               2.449500e-01  4.158141e-01   \nmin                               0.000000e+00  0.000000e+00   \n25%                               0.000000e+00  0.000000e+00   \n50%                               0.000000e+00  0.000000e+00   \n75%                               0.000000e+00  0.000000e+00   \nmax                               1.000000e+00  1.000000e+00   \n\n       Wdft_RegionIdentifier  HasDetections  \ncount           1.823550e+06   1.823550e+06  \nmean            8.322673e+00   4.987135e-01  \nstd             4.477974e+00   4.999985e-01  \nmin             1.000000e+00   0.000000e+00  \n25%             4.000000e+00   0.000000e+00  \n50%             1.000000e+01   0.000000e+00  \n75%             1.100000e+01   1.000000e+00  \nmax             1.500000e+01   1.000000e+00  \n\n[8 rows x 51 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>IsBeta</th>\n      <th>RtpStateBitfield</th>\n      <th>IsSxsPassiveMode</th>\n      <th>AVProductStatesIdentifier</th>\n      <th>AVProductsInstalled</th>\n      <th>AVProductsEnabled</th>\n      <th>HasTpm</th>\n      <th>CountryIdentifier</th>\n      <th>CityIdentifier</th>\n      <th>OrganizationIdentifier</th>\n      <th>...</th>\n      <th>Census_FirmwareVersionIdentifier</th>\n      <th>Census_IsSecureBootEnabled</th>\n      <th>Census_IsWIMBootEnabled</th>\n      <th>Census_IsVirtualDevice</th>\n      <th>Census_IsTouchEnabled</th>\n      <th>Census_IsPenCapable</th>\n      <th>Census_IsAlwaysOnAlwaysConnectedCapable</th>\n      <th>Wdft_IsGamer</th>\n      <th>Wdft_RegionIdentifier</th>\n      <th>HasDetections</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>1.823550e+06</td>\n      <td>1.823550e+06</td>\n      <td>1.823550e+06</td>\n      <td>1.823550e+06</td>\n      <td>1.823550e+06</td>\n      <td>1.823550e+06</td>\n      <td>1.823550e+06</td>\n      <td>1.823550e+06</td>\n      <td>1.823550e+06</td>\n      <td>1.823550e+06</td>\n      <td>...</td>\n      <td>1.823550e+06</td>\n      <td>1.823550e+06</td>\n      <td>1823550.0</td>\n      <td>1.823550e+06</td>\n      <td>1.823550e+06</td>\n      <td>1.823550e+06</td>\n      <td>1.823550e+06</td>\n      <td>1.823550e+06</td>\n      <td>1.823550e+06</td>\n      <td>1.823550e+06</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>2.193524e-06</td>\n      <td>6.847975e+00</td>\n      <td>1.676017e-02</td>\n      <td>4.701612e+04</td>\n      <td>1.383484e+00</td>\n      <td>1.026553e+00</td>\n      <td>9.989625e-01</td>\n      <td>1.069236e+02</td>\n      <td>8.060299e+04</td>\n      <td>2.484157e+01</td>\n      <td>...</td>\n      <td>3.413802e+04</td>\n      <td>5.144592e-01</td>\n      <td>0.0</td>\n      <td>1.355598e-03</td>\n      <td>1.460031e-01</td>\n      <td>3.628362e-02</td>\n      <td>6.411066e-02</td>\n      <td>2.223334e-01</td>\n      <td>8.322673e+00</td>\n      <td>4.987135e-01</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>1.481054e-03</td>\n      <td>1.006668e+00</td>\n      <td>1.283716e-01</td>\n      <td>1.470503e+04</td>\n      <td>5.430370e-01</td>\n      <td>1.835109e-01</td>\n      <td>3.219411e-02</td>\n      <td>6.277194e+01</td>\n      <td>4.913968e+04</td>\n      <td>5.195289e+00</td>\n      <td>...</td>\n      <td>2.095789e+04</td>\n      <td>4.997910e-01</td>\n      <td>0.0</td>\n      <td>3.679349e-02</td>\n      <td>3.531094e-01</td>\n      <td>1.869950e-01</td>\n      <td>2.449500e-01</td>\n      <td>4.158141e-01</td>\n      <td>4.477974e+00</td>\n      <td>4.999985e-01</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>7.000000e+00</td>\n      <td>1.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>1.000000e+00</td>\n      <td>7.000000e+00</td>\n      <td>1.000000e+00</td>\n      <td>...</td>\n      <td>8.700000e+01</td>\n      <td>0.000000e+00</td>\n      <td>0.0</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>1.000000e+00</td>\n      <td>0.000000e+00</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.000000e+00</td>\n      <td>7.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>4.681000e+04</td>\n      <td>1.000000e+00</td>\n      <td>1.000000e+00</td>\n      <td>1.000000e+00</td>\n      <td>5.100000e+01</td>\n      <td>3.480900e+04</td>\n      <td>1.800000e+01</td>\n      <td>...</td>\n      <td>1.704100e+04</td>\n      <td>0.000000e+00</td>\n      <td>0.0</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>4.000000e+00</td>\n      <td>0.000000e+00</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.000000e+00</td>\n      <td>7.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>5.344700e+04</td>\n      <td>1.000000e+00</td>\n      <td>1.000000e+00</td>\n      <td>1.000000e+00</td>\n      <td>9.400000e+01</td>\n      <td>8.237300e+04</td>\n      <td>2.700000e+01</td>\n      <td>...</td>\n      <td>3.307000e+04</td>\n      <td>1.000000e+00</td>\n      <td>0.0</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>1.000000e+01</td>\n      <td>0.000000e+00</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.000000e+00</td>\n      <td>7.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>5.344700e+04</td>\n      <td>2.000000e+00</td>\n      <td>1.000000e+00</td>\n      <td>1.000000e+00</td>\n      <td>1.600000e+02</td>\n      <td>1.241500e+05</td>\n      <td>2.700000e+01</td>\n      <td>...</td>\n      <td>5.289700e+04</td>\n      <td>1.000000e+00</td>\n      <td>0.0</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>1.100000e+01</td>\n      <td>1.000000e+00</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1.000000e+00</td>\n      <td>8.000000e+00</td>\n      <td>1.000000e+00</td>\n      <td>7.049800e+04</td>\n      <td>6.000000e+00</td>\n      <td>5.000000e+00</td>\n      <td>1.000000e+00</td>\n      <td>2.220000e+02</td>\n      <td>1.679620e+05</td>\n      <td>5.200000e+01</td>\n      <td>...</td>\n      <td>7.209600e+04</td>\n      <td>1.000000e+00</td>\n      <td>0.0</td>\n      <td>1.000000e+00</td>\n      <td>1.000000e+00</td>\n      <td>1.000000e+00</td>\n      <td>1.000000e+00</td>\n      <td>1.000000e+00</td>\n      <td>1.500000e+01</td>\n      <td>1.000000e+00</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows × 51 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Data Cleaning","metadata":{}},{"cell_type":"code","source":"data.isna()","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:15:07.623410Z","iopub.execute_input":"2024-06-13T10:15:07.623722Z","iopub.status.idle":"2024-06-13T10:15:10.120906Z","shell.execute_reply.started":"2024-06-13T10:15:07.623698Z","shell.execute_reply":"2024-06-13T10:15:10.119522Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"         MachineIdentifier  ProductName  EngineVersion  AppVersion  \\\n0                    False        False          False       False   \n1                    False        False          False       False   \n2                    False        False          False       False   \n3                    False        False          False       False   \n4                    False        False          False       False   \n...                    ...          ...            ...         ...   \n1823545              False        False          False       False   \n1823546              False        False          False       False   \n1823547              False        False          False       False   \n1823548              False        False          False       False   \n1823549              False        False          False       False   \n\n         AvSigVersion  IsBeta  RtpStateBitfield  IsSxsPassiveMode  \\\n0               False   False             False             False   \n1               False   False             False             False   \n2               False   False             False             False   \n3               False   False             False             False   \n4               False   False             False             False   \n...               ...     ...               ...               ...   \n1823545         False   False             False             False   \n1823546         False   False             False             False   \n1823547         False   False             False             False   \n1823548         False   False             False             False   \n1823549         False   False             False             False   \n\n         AVProductStatesIdentifier  AVProductsInstalled  ...  \\\n0                            False                False  ...   \n1                            False                False  ...   \n2                            False                False  ...   \n3                            False                False  ...   \n4                            False                False  ...   \n...                            ...                  ...  ...   \n1823545                      False                False  ...   \n1823546                      False                False  ...   \n1823547                      False                False  ...   \n1823548                      False                False  ...   \n1823549                      False                False  ...   \n\n         Census_FirmwareVersionIdentifier  Census_IsSecureBootEnabled  \\\n0                                   False                       False   \n1                                   False                       False   \n2                                   False                       False   \n3                                   False                       False   \n4                                   False                       False   \n...                                   ...                         ...   \n1823545                             False                       False   \n1823546                             False                       False   \n1823547                             False                       False   \n1823548                             False                       False   \n1823549                             False                       False   \n\n         Census_IsWIMBootEnabled  Census_IsVirtualDevice  \\\n0                          False                   False   \n1                          False                   False   \n2                          False                   False   \n3                          False                   False   \n4                          False                   False   \n...                          ...                     ...   \n1823545                    False                   False   \n1823546                    False                   False   \n1823547                    False                   False   \n1823548                    False                   False   \n1823549                    False                   False   \n\n         Census_IsTouchEnabled  Census_IsPenCapable  \\\n0                        False                False   \n1                        False                False   \n2                        False                False   \n3                        False                False   \n4                        False                False   \n...                        ...                  ...   \n1823545                  False                False   \n1823546                  False                False   \n1823547                  False                False   \n1823548                  False                False   \n1823549                  False                False   \n\n         Census_IsAlwaysOnAlwaysConnectedCapable  Wdft_IsGamer  \\\n0                                          False         False   \n1                                          False         False   \n2                                          False         False   \n3                                          False         False   \n4                                          False         False   \n...                                          ...           ...   \n1823545                                    False         False   \n1823546                                    False         False   \n1823547                                    False         False   \n1823548                                    False         False   \n1823549                                    False         False   \n\n         Wdft_RegionIdentifier  HasDetections  \n0                        False          False  \n1                        False          False  \n2                        False          False  \n3                        False          False  \n4                        False          False  \n...                        ...            ...  \n1823545                  False          False  \n1823546                  False          False  \n1823547                  False          False  \n1823548                  False          False  \n1823549                  False          False  \n\n[1823550 rows x 78 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MachineIdentifier</th>\n      <th>ProductName</th>\n      <th>EngineVersion</th>\n      <th>AppVersion</th>\n      <th>AvSigVersion</th>\n      <th>IsBeta</th>\n      <th>RtpStateBitfield</th>\n      <th>IsSxsPassiveMode</th>\n      <th>AVProductStatesIdentifier</th>\n      <th>AVProductsInstalled</th>\n      <th>...</th>\n      <th>Census_FirmwareVersionIdentifier</th>\n      <th>Census_IsSecureBootEnabled</th>\n      <th>Census_IsWIMBootEnabled</th>\n      <th>Census_IsVirtualDevice</th>\n      <th>Census_IsTouchEnabled</th>\n      <th>Census_IsPenCapable</th>\n      <th>Census_IsAlwaysOnAlwaysConnectedCapable</th>\n      <th>Wdft_IsGamer</th>\n      <th>Wdft_RegionIdentifier</th>\n      <th>HasDetections</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1823545</th>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1823546</th>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1823547</th>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1823548</th>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1823549</th>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n<p>1823550 rows × 78 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# data[\"Census_IsAlwaysOnAlwaysConnectedCapable\"].isnull().sum() / len(data)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:15:10.122699Z","iopub.execute_input":"2024-06-13T10:15:10.123203Z","iopub.status.idle":"2024-06-13T10:15:10.128383Z","shell.execute_reply.started":"2024-06-13T10:15:10.123160Z","shell.execute_reply":"2024-06-13T10:15:10.127168Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# data.columns[0]","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:15:10.130025Z","iopub.execute_input":"2024-06-13T10:15:10.130839Z","iopub.status.idle":"2024-06-13T10:15:10.139586Z","shell.execute_reply.started":"2024-06-13T10:15:10.130805Z","shell.execute_reply":"2024-06-13T10:15:10.138400Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# for c in data.columns:\n#     if ((data[c].isnull().sum() * 100) / len(data)) > 70:\n#         data.drop(c, inplace=True, axis = 1)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:15:10.140845Z","iopub.execute_input":"2024-06-13T10:15:10.141212Z","iopub.status.idle":"2024-06-13T10:15:10.152319Z","shell.execute_reply.started":"2024-06-13T10:15:10.141180Z","shell.execute_reply":"2024-06-13T10:15:10.150770Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:15:10.156651Z","iopub.execute_input":"2024-06-13T10:15:10.157080Z","iopub.status.idle":"2024-06-13T10:15:10.183754Z","shell.execute_reply.started":"2024-06-13T10:15:10.157049Z","shell.execute_reply":"2024-06-13T10:15:10.182725Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"                  MachineIdentifier   ProductName EngineVersion  \\\n0  000016191b897145d069102325cab760  win8defender   1.1.15100.1   \n1  000019515bc8f95851aff6de873405e8  win8defender   1.1.15100.1   \n2  00001a027a0ab970c408182df8484fce  win8defender   1.1.15200.1   \n3  000027c68b89acb49d4017763b043449  win8defender   1.1.15200.1   \n4  000038f24a1ee98931456b3e49f7934b  win8defender   1.1.15200.1   \n\n        AppVersion  AvSigVersion  IsBeta  RtpStateBitfield  IsSxsPassiveMode  \\\n0  4.18.1807.18075  1.273.1094.0       0               7.0                 0   \n1  4.18.1807.18075  1.273.1393.0       0               7.0                 0   \n2  4.18.1807.18075   1.275.988.0       0               7.0                 0   \n3  4.18.1807.18075   1.275.130.0       0               7.0                 0   \n4  4.18.1806.18062   1.275.879.0       0               7.0                 0   \n\n   AVProductStatesIdentifier  AVProductsInstalled  ...  \\\n0                    53447.0                  1.0  ...   \n1                    53447.0                  1.0  ...   \n2                    53447.0                  1.0  ...   \n3                    47238.0                  2.0  ...   \n4                    53447.0                  1.0  ...   \n\n   Census_FirmwareVersionIdentifier  Census_IsSecureBootEnabled  \\\n0                           51039.0                           0   \n1                           63122.0                           0   \n2                           15510.0                           0   \n3                           19951.0                           1   \n4                           51032.0                           0   \n\n   Census_IsWIMBootEnabled  Census_IsVirtualDevice  Census_IsTouchEnabled  \\\n0                      0.0                     0.0                      0   \n1                      0.0                     0.0                      0   \n2                      0.0                     0.0                      0   \n3                      0.0                     0.0                      0   \n4                      0.0                     0.0                      0   \n\n   Census_IsPenCapable  Census_IsAlwaysOnAlwaysConnectedCapable Wdft_IsGamer  \\\n0                    0                                      0.0          0.0   \n1                    0                                      0.0          0.0   \n2                    0                                      0.0          0.0   \n3                    0                                      0.0          0.0   \n4                    0                                      0.0          0.0   \n\n  Wdft_RegionIdentifier HasDetections  \n0                  15.0             1  \n1                  15.0             0  \n2                  15.0             0  \n3                   1.0             1  \n4                   7.0             0  \n\n[5 rows x 78 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MachineIdentifier</th>\n      <th>ProductName</th>\n      <th>EngineVersion</th>\n      <th>AppVersion</th>\n      <th>AvSigVersion</th>\n      <th>IsBeta</th>\n      <th>RtpStateBitfield</th>\n      <th>IsSxsPassiveMode</th>\n      <th>AVProductStatesIdentifier</th>\n      <th>AVProductsInstalled</th>\n      <th>...</th>\n      <th>Census_FirmwareVersionIdentifier</th>\n      <th>Census_IsSecureBootEnabled</th>\n      <th>Census_IsWIMBootEnabled</th>\n      <th>Census_IsVirtualDevice</th>\n      <th>Census_IsTouchEnabled</th>\n      <th>Census_IsPenCapable</th>\n      <th>Census_IsAlwaysOnAlwaysConnectedCapable</th>\n      <th>Wdft_IsGamer</th>\n      <th>Wdft_RegionIdentifier</th>\n      <th>HasDetections</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>000016191b897145d069102325cab760</td>\n      <td>win8defender</td>\n      <td>1.1.15100.1</td>\n      <td>4.18.1807.18075</td>\n      <td>1.273.1094.0</td>\n      <td>0</td>\n      <td>7.0</td>\n      <td>0</td>\n      <td>53447.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>51039.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>15.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000019515bc8f95851aff6de873405e8</td>\n      <td>win8defender</td>\n      <td>1.1.15100.1</td>\n      <td>4.18.1807.18075</td>\n      <td>1.273.1393.0</td>\n      <td>0</td>\n      <td>7.0</td>\n      <td>0</td>\n      <td>53447.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>63122.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>15.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00001a027a0ab970c408182df8484fce</td>\n      <td>win8defender</td>\n      <td>1.1.15200.1</td>\n      <td>4.18.1807.18075</td>\n      <td>1.275.988.0</td>\n      <td>0</td>\n      <td>7.0</td>\n      <td>0</td>\n      <td>53447.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>15510.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>15.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>000027c68b89acb49d4017763b043449</td>\n      <td>win8defender</td>\n      <td>1.1.15200.1</td>\n      <td>4.18.1807.18075</td>\n      <td>1.275.130.0</td>\n      <td>0</td>\n      <td>7.0</td>\n      <td>0</td>\n      <td>47238.0</td>\n      <td>2.0</td>\n      <td>...</td>\n      <td>19951.0</td>\n      <td>1</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>000038f24a1ee98931456b3e49f7934b</td>\n      <td>win8defender</td>\n      <td>1.1.15200.1</td>\n      <td>4.18.1806.18062</td>\n      <td>1.275.879.0</td>\n      <td>0</td>\n      <td>7.0</td>\n      <td>0</td>\n      <td>53447.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>51032.0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>7.0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 78 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# data.isnull().sum(axis = 1)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:15:10.184945Z","iopub.execute_input":"2024-06-13T10:15:10.185284Z","iopub.status.idle":"2024-06-13T10:15:10.192305Z","shell.execute_reply.started":"2024-06-13T10:15:10.185256Z","shell.execute_reply":"2024-06-13T10:15:10.191265Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# data = data[data.isnull().sum(axis = 1) == 0]\n# data.head()","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:15:10.193541Z","iopub.execute_input":"2024-06-13T10:15:10.193847Z","iopub.status.idle":"2024-06-13T10:15:10.204945Z","shell.execute_reply.started":"2024-06-13T10:15:10.193822Z","shell.execute_reply":"2024-06-13T10:15:10.203886Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# print(data.shape)\n# max(data.isnull().sum())","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:15:10.206172Z","iopub.execute_input":"2024-06-13T10:15:10.206478Z","iopub.status.idle":"2024-06-13T10:15:10.218064Z","shell.execute_reply.started":"2024-06-13T10:15:10.206452Z","shell.execute_reply":"2024-06-13T10:15:10.217027Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# data.to_csv(\"Cleaned_malware.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:15:10.219396Z","iopub.execute_input":"2024-06-13T10:15:10.219849Z","iopub.status.idle":"2024-06-13T10:15:10.229600Z","shell.execute_reply.started":"2024-06-13T10:15:10.219818Z","shell.execute_reply":"2024-06-13T10:15:10.228461Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# data[data['Firewall'] == 1.0]['HasDetections'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:15:10.230756Z","iopub.execute_input":"2024-06-13T10:15:10.231199Z","iopub.status.idle":"2024-06-13T10:15:10.241117Z","shell.execute_reply.started":"2024-06-13T10:15:10.231167Z","shell.execute_reply":"2024-06-13T10:15:10.240048Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"code","source":"sns.histplot(data['ProductName'])","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:15:10.242317Z","iopub.execute_input":"2024-06-13T10:15:10.242681Z","iopub.status.idle":"2024-06-13T10:15:13.294445Z","shell.execute_reply.started":"2024-06-13T10:15:10.242653Z","shell.execute_reply":"2024-06-13T10:15:13.293327Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"<Axes: xlabel='ProductName', ylabel='Count'>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"# How many computers had firewall enabled?\nsns.countplot(x = data[data['Firewall'] == 1.0]['HasDetections'])","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:15:13.295909Z","iopub.execute_input":"2024-06-13T10:15:13.296267Z","iopub.status.idle":"2024-06-13T10:15:14.821725Z","shell.execute_reply.started":"2024-06-13T10:15:13.296238Z","shell.execute_reply":"2024-06-13T10:15:14.820514Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"<Axes: xlabel='HasDetections', ylabel='count'>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"# The version of OS used\n(data['Census_OSVersion'].value_counts() > 10000).head()","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:15:14.823153Z","iopub.execute_input":"2024-06-13T10:15:14.823548Z","iopub.status.idle":"2024-06-13T10:15:14.925280Z","shell.execute_reply.started":"2024-06-13T10:15:14.823511Z","shell.execute_reply":"2024-06-13T10:15:14.924164Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"Census_OSVersion\n10.0.17134.228      True\n10.0.16299.431      True\n10.0.17134.165      True\n10.0.10240.17443    True\n10.0.16299.371      True\nName: count, dtype: bool"},"metadata":{}}]},{"cell_type":"code","source":"data[data['HasDetections'] == 1]['Census_TotalPhysicalRAM'].value_counts().head(10)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:15:14.926687Z","iopub.execute_input":"2024-06-13T10:15:14.927033Z","iopub.status.idle":"2024-06-13T10:15:15.636185Z","shell.execute_reply.started":"2024-06-13T10:15:14.927004Z","shell.execute_reply":"2024-06-13T10:15:15.635202Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"Census_TotalPhysicalRAM\n4096.0     455374\n8192.0     195502\n2048.0     125893\n6144.0      51770\n16384.0     35704\n3072.0      16587\n12288.0     15231\n1024.0       5010\n32768.0      3193\n10240.0      1033\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"# How much RAM did infected PCs have?\nplt.figure(figsize=(20, 8))\nplt.tight_layout()\nx = dict(data[data['HasDetections'] == 1]['Census_TotalPhysicalRAM'].value_counts().head(10)).keys()\ny = dict(data[data['HasDetections'] == 1]['Census_TotalPhysicalRAM'].value_counts().head(10)).values()\nsns.barplot(x = list(x), y = list(y))","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:15:15.637569Z","iopub.execute_input":"2024-06-13T10:15:15.637996Z","iopub.status.idle":"2024-06-13T10:15:17.349684Z","shell.execute_reply.started":"2024-06-13T10:15:15.637944Z","shell.execute_reply":"2024-06-13T10:15:17.348521Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"<Axes: >"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 2000x800 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"data[data['HasDetections'] == 1]['HasTpm'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:15:17.351184Z","iopub.execute_input":"2024-06-13T10:15:17.351522Z","iopub.status.idle":"2024-06-13T10:15:18.054009Z","shell.execute_reply.started":"2024-06-13T10:15:17.351494Z","shell.execute_reply":"2024-06-13T10:15:18.052864Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"HasTpm\n1    908552\n0       877\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"# How many infected PCs had TPM?\nplt.tight_layout()\nx = dict(data[data['HasDetections'] == 1]['HasTpm'].value_counts()).keys()\ny = dict(data[data['HasDetections'] == 1]['HasTpm'].value_counts()).values()\nsns.barplot(x = list(x), y = list(y))","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:15:18.055550Z","iopub.execute_input":"2024-06-13T10:15:18.055859Z","iopub.status.idle":"2024-06-13T10:15:19.600564Z","shell.execute_reply.started":"2024-06-13T10:15:18.055834Z","shell.execute_reply":"2024-06-13T10:15:19.599549Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"<Axes: >"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"data['Wdft_IsGamer'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:15:19.602236Z","iopub.execute_input":"2024-06-13T10:15:19.602541Z","iopub.status.idle":"2024-06-13T10:15:19.629276Z","shell.execute_reply.started":"2024-06-13T10:15:19.602514Z","shell.execute_reply":"2024-06-13T10:15:19.628100Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"Wdft_IsGamer\n0.0    1418114\n1.0     405436\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"# How many infected PCs had gamer device configuration?\nplt.tight_layout()\nx = dict(data[data['HasDetections'] == 1]['Wdft_IsGamer'].value_counts()).keys()\ny = dict(data[data['HasDetections'] == 1]['Wdft_IsGamer'].value_counts()).values()\nsns.barplot(x = list(x), y = list(y))","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:15:19.630797Z","iopub.execute_input":"2024-06-13T10:15:19.631498Z","iopub.status.idle":"2024-06-13T10:15:21.204140Z","shell.execute_reply.started":"2024-06-13T10:15:19.631458Z","shell.execute_reply":"2024-06-13T10:15:21.202764Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"<Axes: >"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"# How many infected computers had any antivirus protection?\ndata[data['HasDetections'] == 1]['IsProtected'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:15:21.205749Z","iopub.execute_input":"2024-06-13T10:15:21.206094Z","iopub.status.idle":"2024-06-13T10:15:21.909456Z","shell.execute_reply.started":"2024-06-13T10:15:21.206066Z","shell.execute_reply":"2024-06-13T10:15:21.908358Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"IsProtected\n1.0    874664\n0.0     34765\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"# Status of Microsoft Smartscreen on the infected computers\ndata[data['HasDetections'] == 1]['SmartScreen'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:15:21.910895Z","iopub.execute_input":"2024-06-13T10:15:21.911333Z","iopub.status.idle":"2024-06-13T10:15:22.647018Z","shell.execute_reply.started":"2024-06-13T10:15:21.911296Z","shell.execute_reply":"2024-06-13T10:15:22.646021Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"SmartScreen\nRequireAdmin    647049\nExistsNotSet    226004\nOff              17147\nWarn             13089\nPrompt            4054\nBlock             1937\n&#x02;              64\noff                 61\n&#x01;              15\nOn                   4\non                   3\nrequireadmin         2\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"# **Data Preprocessing**","metadata":{}},{"cell_type":"markdown","source":"### Label Encoding","metadata":{}},{"cell_type":"code","source":"# data.reset_index(drop = True, inplace = True)\n# data.head()","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:15:22.648334Z","iopub.execute_input":"2024-06-13T10:15:22.648748Z","iopub.status.idle":"2024-06-13T10:15:22.653314Z","shell.execute_reply.started":"2024-06-13T10:15:22.648706Z","shell.execute_reply":"2024-06-13T10:15:22.652310Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# data['AppVersion'].dtypes == 'object'\ndata.dtypes.value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:15:22.660356Z","iopub.execute_input":"2024-06-13T10:15:22.660704Z","iopub.status.idle":"2024-06-13T10:15:22.669354Z","shell.execute_reply.started":"2024-06-13T10:15:22.660676Z","shell.execute_reply":"2024-06-13T10:15:22.668209Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"float64    34\nobject     27\nint64      17\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"len(data['HasDetections'].value_counts())","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:15:22.670604Z","iopub.execute_input":"2024-06-13T10:15:22.671032Z","iopub.status.idle":"2024-06-13T10:15:22.697216Z","shell.execute_reply.started":"2024-06-13T10:15:22.670993Z","shell.execute_reply":"2024-06-13T10:15:22.696139Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"2"},"metadata":{}}]},{"cell_type":"code","source":"le = LabelEncoder()\nnumericCols = []\nfor c in data.columns:\n    if data[c].dtypes != 'object' and len(data[c].value_counts()) > 2:\n        numericCols.append(c)\n    elif data[c].dtypes == 'object':\n        data[c] = le.fit_transform(data[c])\n\nprint(data.dtypes.value_counts())\nprint(numericCols)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:15:22.698352Z","iopub.execute_input":"2024-06-13T10:15:22.698655Z","iopub.status.idle":"2024-06-13T10:15:39.143079Z","shell.execute_reply.started":"2024-06-13T10:15:22.698630Z","shell.execute_reply":"2024-06-13T10:15:39.141863Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"int64      44\nfloat64    34\nName: count, dtype: int64\n['RtpStateBitfield', 'AVProductStatesIdentifier', 'AVProductsInstalled', 'AVProductsEnabled', 'CountryIdentifier', 'CityIdentifier', 'OrganizationIdentifier', 'GeoNameIdentifier', 'LocaleEnglishNameIdentifier', 'OsBuild', 'OsSuite', 'IeVerIdentifier', 'UacLuaenable', 'Census_OEMNameIdentifier', 'Census_OEMModelIdentifier', 'Census_ProcessorCoreCount', 'Census_ProcessorManufacturerIdentifier', 'Census_ProcessorModelIdentifier', 'Census_PrimaryDiskTotalCapacity', 'Census_SystemVolumeTotalCapacity', 'Census_TotalPhysicalRAM', 'Census_InternalPrimaryDiagonalDisplaySizeInInches', 'Census_InternalPrimaryDisplayResolutionHorizontal', 'Census_InternalPrimaryDisplayResolutionVertical', 'Census_InternalBatteryNumberOfCharges', 'Census_OSBuildNumber', 'Census_OSBuildRevision', 'Census_OSInstallLanguageIdentifier', 'Census_OSUILocaleIdentifier', 'Census_FirmwareManufacturerIdentifier', 'Census_FirmwareVersionIdentifier', 'Wdft_RegionIdentifier']\n","output_type":"stream"}]},{"cell_type":"code","source":"# for c in data.columns:\n#     if data[c].dtypes == 'float64':\n#         print(data[c])","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:15:39.144192Z","iopub.execute_input":"2024-06-13T10:15:39.144607Z","iopub.status.idle":"2024-06-13T10:15:39.149938Z","shell.execute_reply.started":"2024-06-13T10:15:39.144570Z","shell.execute_reply":"2024-06-13T10:15:39.148778Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"### Standard Scaling","metadata":{}},{"cell_type":"code","source":"# df_copy = data.copy()\n# df_copy = df_copy.drop(columns=['HasDetections'])\n# scaler = StandardScaler()\n# data_scaled = pd.DataFrame(scaler.fit_transform(df_copy), columns=df_copy.columns)\n# data_scaled['HasDetections'] = data['HasDetections']","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:15:39.151992Z","iopub.execute_input":"2024-06-13T10:15:39.152690Z","iopub.status.idle":"2024-06-13T10:15:39.162610Z","shell.execute_reply.started":"2024-06-13T10:15:39.152648Z","shell.execute_reply":"2024-06-13T10:15:39.161450Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"# for c in numericCols:\n#     data[c] = scaler.fit_transform(data[c].values.reshape(-1, 1))","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:15:39.163740Z","iopub.execute_input":"2024-06-13T10:15:39.164106Z","iopub.status.idle":"2024-06-13T10:15:39.181470Z","shell.execute_reply.started":"2024-06-13T10:15:39.164075Z","shell.execute_reply":"2024-06-13T10:15:39.180094Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"# data.head()","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:15:39.183098Z","iopub.execute_input":"2024-06-13T10:15:39.183552Z","iopub.status.idle":"2024-06-13T10:15:39.195272Z","shell.execute_reply.started":"2024-06-13T10:15:39.183511Z","shell.execute_reply":"2024-06-13T10:15:39.194191Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"### Normalisation","metadata":{}},{"cell_type":"code","source":"list(data.columns)[:-1]","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:15:39.196692Z","iopub.execute_input":"2024-06-13T10:15:39.197134Z","iopub.status.idle":"2024-06-13T10:15:39.211100Z","shell.execute_reply.started":"2024-06-13T10:15:39.197096Z","shell.execute_reply":"2024-06-13T10:15:39.209992Z"},"trusted":true},"execution_count":35,"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"['MachineIdentifier',\n 'ProductName',\n 'EngineVersion',\n 'AppVersion',\n 'AvSigVersion',\n 'IsBeta',\n 'RtpStateBitfield',\n 'IsSxsPassiveMode',\n 'AVProductStatesIdentifier',\n 'AVProductsInstalled',\n 'AVProductsEnabled',\n 'HasTpm',\n 'CountryIdentifier',\n 'CityIdentifier',\n 'OrganizationIdentifier',\n 'GeoNameIdentifier',\n 'LocaleEnglishNameIdentifier',\n 'Platform',\n 'Processor',\n 'OsVer',\n 'OsBuild',\n 'OsSuite',\n 'OsPlatformSubRelease',\n 'OsBuildLab',\n 'SkuEdition',\n 'IsProtected',\n 'AutoSampleOptIn',\n 'SMode',\n 'IeVerIdentifier',\n 'SmartScreen',\n 'Firewall',\n 'UacLuaenable',\n 'Census_MDC2FormFactor',\n 'Census_DeviceFamily',\n 'Census_OEMNameIdentifier',\n 'Census_OEMModelIdentifier',\n 'Census_ProcessorCoreCount',\n 'Census_ProcessorManufacturerIdentifier',\n 'Census_ProcessorModelIdentifier',\n 'Census_PrimaryDiskTotalCapacity',\n 'Census_PrimaryDiskTypeName',\n 'Census_SystemVolumeTotalCapacity',\n 'Census_HasOpticalDiskDrive',\n 'Census_TotalPhysicalRAM',\n 'Census_ChassisTypeName',\n 'Census_InternalPrimaryDiagonalDisplaySizeInInches',\n 'Census_InternalPrimaryDisplayResolutionHorizontal',\n 'Census_InternalPrimaryDisplayResolutionVertical',\n 'Census_PowerPlatformRoleName',\n 'Census_InternalBatteryNumberOfCharges',\n 'Census_OSVersion',\n 'Census_OSArchitecture',\n 'Census_OSBranch',\n 'Census_OSBuildNumber',\n 'Census_OSBuildRevision',\n 'Census_OSEdition',\n 'Census_OSSkuName',\n 'Census_OSInstallTypeName',\n 'Census_OSInstallLanguageIdentifier',\n 'Census_OSUILocaleIdentifier',\n 'Census_OSWUAutoUpdateOptionsName',\n 'Census_IsPortableOperatingSystem',\n 'Census_GenuineStateName',\n 'Census_ActivationChannel',\n 'Census_IsFlightsDisabled',\n 'Census_FlightRing',\n 'Census_ThresholdOptIn',\n 'Census_FirmwareManufacturerIdentifier',\n 'Census_FirmwareVersionIdentifier',\n 'Census_IsSecureBootEnabled',\n 'Census_IsWIMBootEnabled',\n 'Census_IsVirtualDevice',\n 'Census_IsTouchEnabled',\n 'Census_IsPenCapable',\n 'Census_IsAlwaysOnAlwaysConnectedCapable',\n 'Wdft_IsGamer',\n 'Wdft_RegionIdentifier']"},"metadata":{}}]},{"cell_type":"code","source":"# data_norm = pd.DataFrame(normalize(data.drop(\"HasDetections\", axis = 1)), columns=list(data.columns)[:-1])\n# data_norm[\"HasDetections\"] = data[\"HasDetections\"]\n# print(data_norm.shape)\n# data_norm.head()","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:15:39.212453Z","iopub.execute_input":"2024-06-13T10:15:39.212801Z","iopub.status.idle":"2024-06-13T10:15:39.221422Z","shell.execute_reply.started":"2024-06-13T10:15:39.212773Z","shell.execute_reply":"2024-06-13T10:15:39.220359Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"# data_norm[\"HasDetections\"].isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:15:39.222821Z","iopub.execute_input":"2024-06-13T10:15:39.223263Z","iopub.status.idle":"2024-06-13T10:15:39.232208Z","shell.execute_reply.started":"2024-06-13T10:15:39.223224Z","shell.execute_reply":"2024-06-13T10:15:39.231093Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"# Feature Engineering","metadata":{}},{"cell_type":"code","source":"# from sklearn.decomposition import PCA\n# from sklearn.preprocessing import StandardScaler\n# import pandas as pd\n# import numpy as np\n\n# # Assuming df is your DataFrame and 'target' is the column you're interested in\n# X = data.drop('HasDetections', axis=1)\n# y = data['HasDetections']\n\n# # Standardizing the features\n# # X = StandardScaler().fit_transform(X)\n\n# # Perform PCA\n# pca = PCA(n_components=50)\n# principalComponents = pca.fit_transform(X)\n\n# # Create a DataFrame with the principal components\n# principalDf = pd.DataFrame(data = principalComponents, columns = ['Principal Component ' + str(i) for i in range(1, 51)])\n\n# # Concatenate the target column\n# finalDf = pd.concat([principalDf, data[['HasDetections']]], axis = 1)\n\n# # print(finalDf)HasDetections\n# finalDf.head()","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:15:39.233658Z","iopub.execute_input":"2024-06-13T10:15:39.234099Z","iopub.status.idle":"2024-06-13T10:15:39.244795Z","shell.execute_reply.started":"2024-06-13T10:15:39.234060Z","shell.execute_reply":"2024-06-13T10:15:39.243653Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"# print(len(list(top_features)))","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:15:39.246134Z","iopub.execute_input":"2024-06-13T10:15:39.246558Z","iopub.status.idle":"2024-06-13T10:15:39.257320Z","shell.execute_reply.started":"2024-06-13T10:15:39.246519Z","shell.execute_reply":"2024-06-13T10:15:39.256200Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"# dataPCA = pd.DataFrame()\n# for c in tqdm(list(top_features)):\n#     dataPCA[c] = data[c]\n\n# dataPCA['HasDetections'] = data['HasDetections']","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:15:39.258562Z","iopub.execute_input":"2024-06-13T10:15:39.258907Z","iopub.status.idle":"2024-06-13T10:15:39.269176Z","shell.execute_reply.started":"2024-06-13T10:15:39.258876Z","shell.execute_reply":"2024-06-13T10:15:39.268243Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"# dataPCA.head()","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:15:39.270474Z","iopub.execute_input":"2024-06-13T10:15:39.270788Z","iopub.status.idle":"2024-06-13T10:15:39.280698Z","shell.execute_reply.started":"2024-06-13T10:15:39.270756Z","shell.execute_reply":"2024-06-13T10:15:39.279745Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"# df = pd.DataFrame()\n# l = []\n# # data_scaled[data_scaled['HasDetections'] == 1]\n# l.append(data_scaled[data_scaled['HasDetections'] == 1].sample(n = 50000))\n# l.append(data_scaled[data_scaled['HasDetections'] == 0].sample(n = 50000))\n# df = pd.concat(l)\n# df.head()\n# df.shape","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:15:39.281915Z","iopub.execute_input":"2024-06-13T10:15:39.282260Z","iopub.status.idle":"2024-06-13T10:15:39.292615Z","shell.execute_reply.started":"2024-06-13T10:15:39.282233Z","shell.execute_reply":"2024-06-13T10:15:39.291372Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"# df = pd.DataFrame()\n# l = []\n# # data_scaled[data_scaled['HasDetections'] == 1]\n# l.append(data_norm[data_norm['HasDetections'] == 1].sample(n = 100000))\n# l.append(data_norm[data_norm['HasDetections'] == 0].sample(n = 100000))\n# df = pd.concat(l)\n# df.head()\n# df.shape","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:15:39.294255Z","iopub.execute_input":"2024-06-13T10:15:39.294636Z","iopub.status.idle":"2024-06-13T10:15:39.305452Z","shell.execute_reply.started":"2024-06-13T10:15:39.294607Z","shell.execute_reply":"2024-06-13T10:15:39.304320Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame()\nl = []\n# data_scaled[data_scaled['HasDetections'] == 1]\nl.append(data[data['HasDetections'] == 1].sample(n = 50000))\nl.append(data[data['HasDetections'] == 0].sample(n = 50000))\ndf = pd.concat(l)\ndf.head()\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:15:39.307385Z","iopub.execute_input":"2024-06-13T10:15:39.307808Z","iopub.status.idle":"2024-06-13T10:15:40.275386Z","shell.execute_reply.started":"2024-06-13T10:15:39.307770Z","shell.execute_reply":"2024-06-13T10:15:40.274267Z"},"trusted":true},"execution_count":44,"outputs":[{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"(100000, 78)"},"metadata":{}}]},{"cell_type":"code","source":"df['HasDetections'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:15:40.276604Z","iopub.execute_input":"2024-06-13T10:15:40.276937Z","iopub.status.idle":"2024-06-13T10:15:40.286186Z","shell.execute_reply.started":"2024-06-13T10:15:40.276908Z","shell.execute_reply":"2024-06-13T10:15:40.285037Z"},"trusted":true},"execution_count":45,"outputs":[{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"HasDetections\n1    50000\n0    50000\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# len(df[0])","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:15:40.287497Z","iopub.execute_input":"2024-06-13T10:15:40.287864Z","iopub.status.idle":"2024-06-13T10:15:40.296733Z","shell.execute_reply.started":"2024-06-13T10:15:40.287834Z","shell.execute_reply":"2024-06-13T10:15:40.295689Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"# Splitting the dataset","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:15:40.298356Z","iopub.execute_input":"2024-06-13T10:15:40.298861Z","iopub.status.idle":"2024-06-13T10:15:40.307927Z","shell.execute_reply.started":"2024-06-13T10:15:40.298822Z","shell.execute_reply":"2024-06-13T10:15:40.306777Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"x = df.drop(\"HasDetections\", axis = 1)\ny = df['HasDetections']\n# scaler = StandardScaler()\n# x_scaled = scaler.fit_transform(x)\npca = PCA(n_components=10)\nx_pca = pca.fit_transform(x)\ndf_pca = pd.DataFrame(x_pca, columns=[f'PC{i+1}' for i in range(10)])\ndf_pca['HasDetections'] = y.values\nx = df_pca.drop(\"HasDetections\", axis = 1)\ny = df_pca['HasDetections']\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.30, random_state=42, stratify = y)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:20:29.149134Z","iopub.execute_input":"2024-06-13T10:20:29.150615Z","iopub.status.idle":"2024-06-13T10:20:30.204910Z","shell.execute_reply.started":"2024-06-13T10:20:29.150563Z","shell.execute_reply":"2024-06-13T10:20:30.203991Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"df_pca.shape","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:20:31.258228Z","iopub.execute_input":"2024-06-13T10:20:31.258595Z","iopub.status.idle":"2024-06-13T10:20:31.265979Z","shell.execute_reply.started":"2024-06-13T10:20:31.258568Z","shell.execute_reply":"2024-06-13T10:20:31.264796Z"},"trusted":true},"execution_count":53,"outputs":[{"execution_count":53,"output_type":"execute_result","data":{"text/plain":"(100000, 11)"},"metadata":{}}]},{"cell_type":"markdown","source":"# Models","metadata":{}},{"cell_type":"markdown","source":"#### 1. Linear SVC","metadata":{}},{"cell_type":"code","source":"modelSVC = SVC()\nstart = time()\nmodelSVC.fit(x_train, y_train)\nend = time()\nprint(\"--------------------------------------------------------\")\nprint(f\"Time taken: {end - start} seconds\")\nprint(\"--------------------------------------------------------\")\npred_SVC = modelSVC.predict(x_test)\nacc = accuracy_score(pred_SVC , y_test)\ncm = confusion_matrix(pred_SVC , y_test)\ncr = classification_report(pred_SVC , y_test)\nprint(acc)\nprint(\"--------------------------------------------------------\")\nprint(cm)\nprint(\"--------------------------------------------------------\")\nprint(cr)\nprint(\"--------------------------------------------------------\")","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:12:10.084162Z","iopub.execute_input":"2024-06-13T10:12:10.084583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2. LGBM","metadata":{}},{"cell_type":"code","source":"modelLGBM = LGBMClassifier()\nstart = time()\nmodelLGBM.fit(x_train , y_train)\nend = time()\nprint(\"--------------------------------------------------------\")\nprint(f\"Time taken: {end - start} seconds\")\nprint(\"--------------------------------------------------------\")\npred_LGBM = modelLGBM.predict(x_test)\nacc = accuracy_score(pred_LGBM , y_test)\ncm = confusion_matrix(pred_LGBM , y_test)\ncr = classification_report(pred_LGBM , y_test)\nprint(acc)\nprint(\"--------------------------------------------------------\")\nprint(cm)\nprint(\"--------------------------------------------------------\")\nprint(cr)\nprint(\"--------------------------------------------------------\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgbm = LGBMClassifier()\nparam_grid = {\n    'boosting_type': ['gbdt', 'dart', 'goss'],\n    'num_leaves': [31, 50, 70, 100],\n    'max_depth': [-1, 10, 20, 30],\n    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n    'n_estimators': [100, 200, 500]\n}\ngrid_search = GridSearchCV(estimator=lgbm, param_grid=param_grid, scoring='accuracy', n_jobs=-1, verbose=2, cv = 2)\ngrid_search.fit(x_train, y_train)\nprint(\"Best parameters found: \", grid_search.best_params_)\nbest_model = grid_search.best_estimator_\ny_pred = best_model.predict(x_test)\nprint(classification_report(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2024-06-13T13:13:50.249589Z","iopub.execute_input":"2024-06-13T13:13:50.250063Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Fitting 2 folds for each of 576 candidates, totalling 1152 fits\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018965 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=-1, n_estimators=100, num_leaves=31; total time=   4.0s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031030 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=-1, n_estimators=100, num_leaves=70; total time=   6.2s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037410 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=-1, n_estimators=200, num_leaves=31; total time=   9.0s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037286 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=-1, n_estimators=200, num_leaves=70; total time=  11.6s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037208 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=-1, n_estimators=500, num_leaves=31; total time=  19.3s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018540 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=-1, n_estimators=500, num_leaves=70; total time=  24.3s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037072 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=10, n_estimators=100, num_leaves=31; total time=   4.7s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022151 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=10, n_estimators=100, num_leaves=50; total time=   5.3s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018520 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=10, n_estimators=100, num_leaves=70; total time=   5.6s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018504 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=10, n_estimators=200, num_leaves=31; total time=   8.5s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018523 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=10, n_estimators=200, num_leaves=70; total time=  10.8s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018508 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=10, n_estimators=500, num_leaves=31; total time=  18.5s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006492 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=-1, n_estimators=100, num_leaves=31; total time=   4.0s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018528 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=-1, n_estimators=100, num_leaves=70; total time=   6.0s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007861 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=-1, n_estimators=200, num_leaves=31; total time=   9.0s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037783 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=-1, n_estimators=200, num_leaves=70; total time=  11.5s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037131 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=-1, n_estimators=500, num_leaves=31; total time=  19.4s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018513 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=-1, n_estimators=500, num_leaves=70; total time=  24.7s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037279 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=10, n_estimators=100, num_leaves=31; total time=   4.7s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018541 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=10, n_estimators=100, num_leaves=50; total time=   5.1s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037163 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=10, n_estimators=100, num_leaves=70; total time=   5.8s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022393 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=10, n_estimators=200, num_leaves=31; total time=   8.8s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018519 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=10, n_estimators=200, num_leaves=70; total time=  10.8s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037407 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=10, n_estimators=500, num_leaves=31; total time=  19.5s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015553 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=-1, n_estimators=100, num_leaves=50; total time=   5.2s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018618 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=-1, n_estimators=100, num_leaves=100; total time=   6.9s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018515 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=-1, n_estimators=200, num_leaves=50; total time=  10.0s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022040 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=-1, n_estimators=200, num_leaves=100; total time=  13.3s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018605 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=-1, n_estimators=500, num_leaves=50; total time=  21.7s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037156 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=-1, n_estimators=500, num_leaves=100; total time=  29.7s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037166 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=10, n_estimators=100, num_leaves=100; total time=   6.5s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022082 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=10, n_estimators=200, num_leaves=50; total time=  10.1s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.040254 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=10, n_estimators=200, num_leaves=100; total time=  12.6s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037136 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=10, n_estimators=500, num_leaves=50; total time=  22.9s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022023 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=10, n_estimators=500, num_leaves=100; total time=  32.3s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018512 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014212 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=-1, n_estimators=100, num_leaves=50; total time=   5.4s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018737 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=-1, n_estimators=100, num_leaves=100; total time=   6.9s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018514 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=-1, n_estimators=200, num_leaves=50; total time=  10.0s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037012 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=-1, n_estimators=200, num_leaves=100; total time=  13.3s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022390 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=-1, n_estimators=500, num_leaves=50; total time=  22.5s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018509 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=-1, n_estimators=500, num_leaves=100; total time=  29.3s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037023 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=10, n_estimators=100, num_leaves=100; total time=   6.5s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022028 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=10, n_estimators=200, num_leaves=50; total time=  10.0s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037077 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=10, n_estimators=200, num_leaves=100; total time=  12.5s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037063 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=10, n_estimators=500, num_leaves=50; total time=  22.7s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022258 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=10, n_estimators=500, num_leaves=100; total time=  32.2s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018497 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=20, n_estimators=200, num_leaves=31; total time=   8.6s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018609 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023522 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=10, n_estimators=500, num_leaves=70; total time=  25.2s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022054 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=20, n_estimators=100, num_leaves=31; total time=   4.7s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018512 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=20, n_estimators=100, num_leaves=50; total time=   5.2s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037232 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=20, n_estimators=100, num_leaves=70; total time=   6.1s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022352 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=20, n_estimators=100, num_leaves=100; total time=   6.9s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038064 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=20, n_estimators=200, num_leaves=50; total time=  10.3s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.028207 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=20, n_estimators=200, num_leaves=100; total time=  13.4s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018508 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=20, n_estimators=500, num_leaves=50; total time=  21.6s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018522 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=20, n_estimators=500, num_leaves=100; total time=  29.6s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018523 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=30, n_estimators=200, num_leaves=31; total time=   8.6s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018633 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=30, n_estimators=200, num_leaves=70; total time=  11.2s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018514 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018529 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=10, n_estimators=500, num_leaves=70; total time=  25.1s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037068 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=20, n_estimators=100, num_leaves=31; total time=   4.7s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018543 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=20, n_estimators=100, num_leaves=50; total time=   5.2s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037168 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=20, n_estimators=100, num_leaves=70; total time=   6.1s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022010 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=20, n_estimators=100, num_leaves=100; total time=   6.9s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022120 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=20, n_estimators=200, num_leaves=50; total time=  10.3s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018621 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=20, n_estimators=200, num_leaves=100; total time=  12.7s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018524 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=20, n_estimators=500, num_leaves=50; total time=  21.5s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018505 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=20, n_estimators=500, num_leaves=100; total time=  29.3s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008508 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=30, n_estimators=100, num_leaves=100; total time=   6.7s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018601 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=30, n_estimators=200, num_leaves=50; total time=  10.1s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018557 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=30, n_estimators=200, num_leaves=100; total time=  12.7s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037000 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=30, n_estimators=500, num_leaves=50; total time=  22.2s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=20, n_estimators=200, num_leaves=31; total time=   8.7s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018512 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=20, n_estimators=200, num_leaves=70; total time=  11.1s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018528 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=20, n_estimators=500, num_leaves=31; total time=  18.8s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032185 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=20, n_estimators=500, num_leaves=70; total time=  25.4s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018503 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=30, n_estimators=100, num_leaves=31; total time=   4.6s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022437 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=30, n_estimators=100, num_leaves=50; total time=   5.7s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022225 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=30, n_estimators=100, num_leaves=70; total time=   6.2s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007017 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=30, n_estimators=200, num_leaves=31; total time=   8.9s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022061 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=30, n_estimators=200, num_leaves=70; total time=  11.6s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018514 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=30, n_estimators=500, num_leaves=31; total time=  18.7s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.023162 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=30, n_estimators=500, num_leaves=70; total time=  25.3s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018545 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=20, n_estimators=200, num_leaves=70; total time=  11.1s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018521 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=20, n_estimators=500, num_leaves=31; total time=  18.8s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037069 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=20, n_estimators=500, num_leaves=70; total time=  24.9s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018771 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=30, n_estimators=100, num_leaves=31; total time=   4.6s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018538 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=30, n_estimators=100, num_leaves=50; total time=   5.5s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022389 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=30, n_estimators=100, num_leaves=70; total time=   6.1s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018515 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=30, n_estimators=100, num_leaves=100; total time=   6.7s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037104 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=30, n_estimators=200, num_leaves=50; total time=  10.3s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018658 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=30, n_estimators=200, num_leaves=100; total time=  12.7s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018529 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=30, n_estimators=500, num_leaves=50; total time=  21.8s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037120 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=30, n_estimators=500, num_leaves=100; total time=  30.3s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037359 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=-1, n_estimators=200, num_leaves=31; total time=   6.8s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022245 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=-1, n_estimators=200, num_leaves=70; total time=   9.1s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=-1, n_estimators=100, num_leaves=31; total time=   4.1s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022217 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=-1, n_estimators=100, num_leaves=50; total time=   4.8s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037532 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=-1, n_estimators=100, num_leaves=70; total time=   5.3s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018597 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=-1, n_estimators=200, num_leaves=31; total time=   6.8s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017514 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=-1, n_estimators=200, num_leaves=70; total time=   8.8s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037331 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=-1, n_estimators=500, num_leaves=31; total time=  14.9s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018496 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=-1, n_estimators=500, num_leaves=70; total time=  19.9s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018703 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=10, n_estimators=100, num_leaves=31; total time=   4.1s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018528 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=10, n_estimators=100, num_leaves=50; total time=   4.7s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003785 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=10, n_estimators=100, num_leaves=70; total time=   5.2s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022160 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=10, n_estimators=100, num_leaves=100; total time=   6.2s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018526 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=10, n_estimators=200, num_leaves=50; total time=   7.9s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012403 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=30, n_estimators=500, num_leaves=31; total time=  18.7s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037209 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=30, n_estimators=500, num_leaves=70; total time=  25.1s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018753 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=-1, n_estimators=100, num_leaves=31; total time=   4.1s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022139 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=-1, n_estimators=100, num_leaves=50; total time=   4.7s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037802 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=-1, n_estimators=100, num_leaves=70; total time=   5.3s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022741 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=-1, n_estimators=100, num_leaves=100; total time=   6.2s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021995 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=-1, n_estimators=200, num_leaves=50; total time=   8.1s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018529 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=-1, n_estimators=200, num_leaves=100; total time=  10.3s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018514 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=-1, n_estimators=500, num_leaves=50; total time=  17.6s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037318 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=-1, n_estimators=500, num_leaves=100; total time=  24.6s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018533 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=10, n_estimators=200, num_leaves=31; total time=   6.8s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018794 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=10, n_estimators=200, num_leaves=70; total time=   9.0s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018517 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=10, n_estimators=500, num_leaves=31; total time=  14.8s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018511 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.01, max_depth=30, n_estimators=500, num_leaves=100; total time=  28.7s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018504 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=-1, n_estimators=100, num_leaves=100; total time=   6.0s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039327 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=-1, n_estimators=200, num_leaves=50; total time=   8.0s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037199 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=-1, n_estimators=200, num_leaves=100; total time=  10.6s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018558 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=-1, n_estimators=500, num_leaves=50; total time=  17.2s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018518 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=-1, n_estimators=500, num_leaves=100; total time=  23.9s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018527 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=10, n_estimators=100, num_leaves=100; total time=   6.1s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037073 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=10, n_estimators=200, num_leaves=50; total time=   8.1s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037069 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=10, n_estimators=200, num_leaves=100; total time=  11.2s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017040 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=10, n_estimators=500, num_leaves=50; total time=  18.2s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022174 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018538 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=-1, n_estimators=500, num_leaves=31; total time=  14.7s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018549 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=-1, n_estimators=500, num_leaves=70; total time=  20.2s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018515 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=10, n_estimators=100, num_leaves=31; total time=   4.2s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018637 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=10, n_estimators=100, num_leaves=50; total time=   4.7s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018833 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=10, n_estimators=100, num_leaves=70; total time=   5.3s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018508 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=10, n_estimators=200, num_leaves=31; total time=   6.9s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018547 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=10, n_estimators=200, num_leaves=70; total time=   9.1s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018723 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=10, n_estimators=500, num_leaves=31; total time=  14.9s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.027322 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=10, n_estimators=500, num_leaves=70; total time=  21.8s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022236 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=20, n_estimators=100, num_leaves=31; total time=   4.2s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=10, n_estimators=200, num_leaves=100; total time=  11.0s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018528 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=10, n_estimators=500, num_leaves=50; total time=  17.8s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018555 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=10, n_estimators=500, num_leaves=100; total time=  29.4s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018542 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=20, n_estimators=200, num_leaves=31; total time=   6.8s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022430 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=20, n_estimators=200, num_leaves=70; total time=   9.3s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014293 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=20, n_estimators=500, num_leaves=31; total time=  15.3s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021974 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=20, n_estimators=500, num_leaves=70; total time=  20.3s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=10, n_estimators=500, num_leaves=100; total time=  30.4s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037348 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=20, n_estimators=200, num_leaves=31; total time=   7.0s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018634 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=20, n_estimators=200, num_leaves=70; total time=   9.3s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032851 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=20, n_estimators=500, num_leaves=31; total time=  15.2s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018499 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=20, n_estimators=500, num_leaves=70; total time=  20.0s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037240 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=10, n_estimators=500, num_leaves=70; total time=  21.8s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018517 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=20, n_estimators=100, num_leaves=31; total time=   4.2s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018511 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=20, n_estimators=100, num_leaves=50; total time=   4.6s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018519 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=20, n_estimators=100, num_leaves=70; total time=   5.2s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037113 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=20, n_estimators=100, num_leaves=100; total time=   6.3s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037186 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=20, n_estimators=200, num_leaves=50; total time=   8.0s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022436 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=20, n_estimators=200, num_leaves=100; total time=  11.1s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037609 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=20, n_estimators=500, num_leaves=50; total time=  17.6s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037334 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=20, n_estimators=500, num_leaves=100; total time=  24.1s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037183 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=30, n_estimators=100, num_leaves=100; total time=   6.2s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037333 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=30, n_estimators=200, num_leaves=50; total time=   8.1s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018911 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=30, n_estimators=200, num_leaves=100; total time=  10.4s\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018532 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=30, n_estimators=100, num_leaves=31; total time=   4.1s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018583 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=30, n_estimators=100, num_leaves=50; total time=   4.7s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021985 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=30, n_estimators=100, num_leaves=70; total time=   5.3s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018514 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=30, n_estimators=200, num_leaves=31; total time=   6.9s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018531 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=30, n_estimators=200, num_leaves=70; total time=   9.0s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022253 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=30, n_estimators=500, num_leaves=31; total time=  14.8s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018549 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=30, n_estimators=500, num_leaves=70; total time=  19.6s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018529 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.1, max_depth=-1, n_estimators=100, num_leaves=31; total time=   3.6s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012157 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.1, max_depth=-1, n_estimators=100, num_leaves=50; total time=   4.1s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037211 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.1, max_depth=-1, n_estimators=100, num_leaves=70; total time=   4.7s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037360 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.1, max_depth=-1, n_estimators=100, num_leaves=100; total time=   5.4s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018557 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.1, max_depth=-1, n_estimators=200, num_leaves=50; total time=   7.2s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022037 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022272 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=30, n_estimators=100, num_leaves=31; total time=   4.2s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018658 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=30, n_estimators=100, num_leaves=50; total time=   4.7s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037181 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=30, n_estimators=100, num_leaves=70; total time=   5.3s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038260 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=30, n_estimators=200, num_leaves=31; total time=   7.0s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037299 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=30, n_estimators=200, num_leaves=70; total time=   9.1s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037250 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=30, n_estimators=500, num_leaves=31; total time=  14.8s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018537 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=30, n_estimators=500, num_leaves=70; total time=  19.8s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018546 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.1, max_depth=-1, n_estimators=100, num_leaves=31; total time=   3.6s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018540 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.1, max_depth=-1, n_estimators=100, num_leaves=50; total time=   4.1s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018526 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.1, max_depth=-1, n_estimators=100, num_leaves=70; total time=   4.6s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018536 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.1, max_depth=-1, n_estimators=200, num_leaves=31; total time=   6.1s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037144 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.1, max_depth=-1, n_estimators=200, num_leaves=70; total time=   8.3s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022026 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=20, n_estimators=100, num_leaves=50; total time=   4.8s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022119 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=20, n_estimators=100, num_leaves=70; total time=   5.4s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037251 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=20, n_estimators=100, num_leaves=100; total time=   6.1s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022227 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=20, n_estimators=200, num_leaves=50; total time=   8.2s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018548 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=20, n_estimators=200, num_leaves=100; total time=  10.9s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018734 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=20, n_estimators=500, num_leaves=50; total time=  17.5s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018538 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=20, n_estimators=500, num_leaves=100; total time=  24.5s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.027982 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=30, n_estimators=100, num_leaves=100; total time=   6.2s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022058 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=30, n_estimators=200, num_leaves=50; total time=   8.2s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037439 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=30, n_estimators=200, num_leaves=100; total time=  10.6s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038022 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=30, n_estimators=500, num_leaves=50; total time=  17.6s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004011 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=30, n_estimators=500, num_leaves=100; total time=  23.8s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018578 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.1, max_depth=-1, n_estimators=200, num_leaves=31; total time=   6.1s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039593 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.1, max_depth=-1, n_estimators=200, num_leaves=70; total time=   8.5s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036263 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.1, max_depth=-1, n_estimators=500, num_leaves=31; total time=  14.0s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022172 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.1, max_depth=-1, n_estimators=500, num_leaves=70; total time=  19.0s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037091 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END boosting_type=gbdt, learning_rate=0.1, max_depth=10, n_estimators=100, num_leaves=31; total time=   3.7s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037008 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.1, max_depth=10, n_estimators=100, num_leaves=50; total time=   4.2s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018671 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[CV] END boosting_type=gbdt, learning_rate=0.1, max_depth=10, n_estimators=100, num_leaves=70; total time=   4.7s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037290 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[CV] END boosting_type=gbdt, learning_rate=0.1, max_depth=10, n_estimators=100, num_leaves=100; total time=   5.7s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022346 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.1, max_depth=10, n_estimators=200, num_leaves=50; total time=   7.4s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022032 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[CV] END boosting_type=gbdt, learning_rate=0.1, max_depth=10, n_estimators=200, num_leaves=100; total time=  10.7s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037064 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.1, max_depth=10, n_estimators=500, num_leaves=50; total time=  17.3s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.042200 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.1, max_depth=-1, n_estimators=200, num_leaves=100; total time=   9.8s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.026308 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.1, max_depth=-1, n_estimators=500, num_leaves=50; total time=  16.5s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018502 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.1, max_depth=-1, n_estimators=500, num_leaves=100; total time=  22.8s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018528 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[CV] END boosting_type=gbdt, learning_rate=0.1, max_depth=10, n_estimators=100, num_leaves=100; total time=   5.6s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019540 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.1, max_depth=10, n_estimators=200, num_leaves=50; total time=   7.4s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018547 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[CV] END boosting_type=gbdt, learning_rate=0.1, max_depth=10, n_estimators=200, num_leaves=100; total time=  10.4s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022301 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.1, max_depth=10, n_estimators=500, num_leaves=50; total time=  17.1s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037272 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037492 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=30, n_estimators=500, num_leaves=50; total time=  17.4s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018564 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.05, max_depth=30, n_estimators=500, num_leaves=100; total time=  23.9s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037344 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.1, max_depth=-1, n_estimators=100, num_leaves=100; total time=   5.4s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018528 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.1, max_depth=-1, n_estimators=200, num_leaves=50; total time=   7.2s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037453 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.1, max_depth=-1, n_estimators=200, num_leaves=100; total time=   9.9s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018510 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.1, max_depth=-1, n_estimators=500, num_leaves=50; total time=  16.4s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037139 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.1, max_depth=-1, n_estimators=500, num_leaves=100; total time=  23.6s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022175 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END boosting_type=gbdt, learning_rate=0.1, max_depth=10, n_estimators=200, num_leaves=31; total time=   6.3s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018552 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[CV] END boosting_type=gbdt, learning_rate=0.1, max_depth=10, n_estimators=200, num_leaves=70; total time=   8.6s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.024261 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END boosting_type=gbdt, learning_rate=0.1, max_depth=10, n_estimators=500, num_leaves=31; total time=  14.3s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018566 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[CV] END boosting_type=gbdt, learning_rate=0.1, max_depth=10, n_estimators=500, num_leaves=70; total time=  20.6s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018531 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018554 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.1, max_depth=-1, n_estimators=500, num_leaves=31; total time=  13.9s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018539 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.1, max_depth=-1, n_estimators=500, num_leaves=70; total time=  19.1s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018517 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END boosting_type=gbdt, learning_rate=0.1, max_depth=10, n_estimators=100, num_leaves=31; total time=   3.6s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035106 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.1, max_depth=10, n_estimators=100, num_leaves=50; total time=   4.2s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018695 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.1, max_depth=10, n_estimators=100, num_leaves=70; total time=   4.8s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013636 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END boosting_type=gbdt, learning_rate=0.1, max_depth=10, n_estimators=200, num_leaves=31; total time=   6.3s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037124 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[CV] END boosting_type=gbdt, learning_rate=0.1, max_depth=10, n_estimators=200, num_leaves=70; total time=   8.7s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018545 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END boosting_type=gbdt, learning_rate=0.1, max_depth=10, n_estimators=500, num_leaves=31; total time=  14.0s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037054 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[CV] END boosting_type=gbdt, learning_rate=0.1, max_depth=10, n_estimators=500, num_leaves=70; total time=  20.8s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022088 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END boosting_type=gbdt, learning_rate=0.1, max_depth=20, n_estimators=100, num_leaves=31; total time=   3.6s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[CV] END boosting_type=gbdt, learning_rate=0.1, max_depth=10, n_estimators=500, num_leaves=100; total time=  29.1s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018498 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.1, max_depth=20, n_estimators=200, num_leaves=50; total time=   7.2s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022146 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.1, max_depth=20, n_estimators=200, num_leaves=100; total time=   9.8s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018514 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.1, max_depth=20, n_estimators=500, num_leaves=50; total time=  16.3s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018508 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.1, max_depth=20, n_estimators=500, num_leaves=100; total time=  23.0s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022114 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.1, max_depth=30, n_estimators=100, num_leaves=100; total time=   5.5s\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END boosting_type=gbdt, learning_rate=0.1, max_depth=20, n_estimators=100, num_leaves=31; total time=   3.6s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018516 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.1, max_depth=20, n_estimators=100, num_leaves=50; total time=   4.1s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022133 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.1, max_depth=20, n_estimators=100, num_leaves=70; total time=   4.5s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022204 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.1, max_depth=20, n_estimators=100, num_leaves=100; total time=   5.5s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037054 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END boosting_type=gbdt, learning_rate=0.1, max_depth=20, n_estimators=200, num_leaves=31; total time=   6.2s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022053 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.1, max_depth=20, n_estimators=200, num_leaves=70; total time=   8.2s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022117 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END boosting_type=gbdt, learning_rate=0.1, max_depth=20, n_estimators=500, num_leaves=31; total time=  14.0s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022052 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.1, max_depth=20, n_estimators=500, num_leaves=70; total time=  19.3s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018790 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END boosting_type=gbdt, learning_rate=0.1, max_depth=30, n_estimators=100, num_leaves=31; total time=   3.6s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018528 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.1, max_depth=30, n_estimators=100, num_leaves=50; total time=   4.1s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022269 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.1, max_depth=30, n_estimators=100, num_leaves=70; total time=   4.8s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003878 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018797 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.1, max_depth=20, n_estimators=100, num_leaves=50; total time=   4.1s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037084 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.1, max_depth=20, n_estimators=100, num_leaves=70; total time=   4.6s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037051 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.1, max_depth=20, n_estimators=100, num_leaves=100; total time=   5.4s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018517 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END boosting_type=gbdt, learning_rate=0.1, max_depth=20, n_estimators=200, num_leaves=31; total time=   6.1s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018524 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.1, max_depth=20, n_estimators=200, num_leaves=70; total time=   8.2s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018516 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END boosting_type=gbdt, learning_rate=0.1, max_depth=20, n_estimators=500, num_leaves=31; total time=  13.9s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022056 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.1, max_depth=20, n_estimators=500, num_leaves=70; total time=  19.1s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018508 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END boosting_type=gbdt, learning_rate=0.1, max_depth=30, n_estimators=100, num_leaves=31; total time=   3.7s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018625 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.1, max_depth=30, n_estimators=100, num_leaves=50; total time=   4.0s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018539 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.1, max_depth=30, n_estimators=100, num_leaves=70; total time=   4.7s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016710 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END boosting_type=gbdt, learning_rate=0.1, max_depth=30, n_estimators=200, num_leaves=31; total time=   6.2s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[CV] END boosting_type=gbdt, learning_rate=0.1, max_depth=10, n_estimators=500, num_leaves=100; total time=  28.5s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022084 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.1, max_depth=20, n_estimators=200, num_leaves=50; total time=   7.2s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022088 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.1, max_depth=20, n_estimators=200, num_leaves=100; total time=   9.7s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018573 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.1, max_depth=20, n_estimators=500, num_leaves=50; total time=  16.3s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018574 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.1, max_depth=20, n_estimators=500, num_leaves=100; total time=  23.0s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018526 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.1, max_depth=30, n_estimators=100, num_leaves=100; total time=   5.4s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037370 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.1, max_depth=30, n_estimators=200, num_leaves=50; total time=   7.3s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022033 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.1, max_depth=30, n_estimators=200, num_leaves=100; total time=   9.7s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022289 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.1, max_depth=30, n_estimators=500, num_leaves=50; total time=  16.5s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018501 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[CV] END boosting_type=gbdt, learning_rate=0.1, max_depth=30, n_estimators=500, num_leaves=100; total time=  22.9s\n[LightGBM] [Info] Number of positive: 17500, number of negative: 17500\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018510 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 10\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### 3. Random Forest","metadata":{}},{"cell_type":"code","source":"model_RF = RandomForestClassifier(n_jobs=-1)\nstart = time()\nmodel_RF.fit(x_train, y_train)\nend = time()\nprint(\"--------------------------------------------------------\")\nprint(f\"Time taken: {end - start} seconds\")\nprint(\"--------------------------------------------------------\")\npred_RF = model_RF.predict(x_test)\nacc = accuracy_score(pred_RF , y_test)\ncm = confusion_matrix(pred_RF , y_test)\ncr = classification_report(pred_RF , y_test)\nprint(acc)\nprint(\"--------------------------------------------------------\")\nprint(cm)\nprint(\"--------------------------------------------------------\")\nprint(cr)\nprint(\"--------------------------------------------------------\")","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:20:36.737897Z","iopub.execute_input":"2024-06-13T10:20:36.738303Z","iopub.status.idle":"2024-06-13T10:20:51.578571Z","shell.execute_reply.started":"2024-06-13T10:20:36.738272Z","shell.execute_reply":"2024-06-13T10:20:51.577473Z"},"trusted":true},"execution_count":54,"outputs":[{"name":"stdout","text":"--------------------------------------------------------\nTime taken: 14.350777864456177 seconds\n--------------------------------------------------------\n0.5832666666666667\n--------------------------------------------------------\n[[8908 6410]\n [6092 8590]]\n--------------------------------------------------------\n              precision    recall  f1-score   support\n\n           0       0.59      0.58      0.59     15318\n           1       0.57      0.59      0.58     14682\n\n    accuracy                           0.58     30000\n   macro avg       0.58      0.58      0.58     30000\nweighted avg       0.58      0.58      0.58     30000\n\n--------------------------------------------------------\n","output_type":"stream"}]},{"cell_type":"code","source":"cv_scores = cross_val_score(model_RF, x, y, cv=5, scoring='accuracy')\nprint(cv_scores)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:20:51.580259Z","iopub.execute_input":"2024-06-13T10:20:51.580575Z","iopub.status.idle":"2024-06-13T10:22:18.998298Z","shell.execute_reply.started":"2024-06-13T10:20:51.580548Z","shell.execute_reply":"2024-06-13T10:22:18.997018Z"},"trusted":true},"execution_count":55,"outputs":[{"name":"stdout","text":"[0.5824  0.5782  0.58055 0.58    0.5843 ]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### 4. ANN","metadata":{}},{"cell_type":"code","source":"# x_lstm = np.array(x)\n# y_lstm = np.array(y)\n# x_lstm = x_lstm.reshape((x_lstm.shape[0], x_lstm.shape[1], 1))\n# x_train1, x_test1, y_train1, y_test1 = train_test_split(x_lstm, y_lstm, test_size=0.30, random_state=42)\n# model = keras.Sequential([\n#     keras.layers.LSTM(256, activation='relu', return_sequences=True),\n#     keras.layers.LSTM(512, activation='relu'),\n# #     keras.layers.Dropout(0.5),\n#     keras.layers.Dense(1, activation='sigmoid')\n# ])\n\n# model.compile(optimizer=keras.optimizers.Adam(), loss=tf.keras.losses.BinaryCrossentropy(), metrics=[keras.metrics.Accuracy()])\n\n# start = time()\n# history = model.fit(x_train1, y_train1, epochs=5, batch_size=32)\n# end = time()\n# print(\"--------------------------------------------------------\")\n# print(f\"Time taken: {end - start} seconds\")\n# print(\"--------------------------------------------------------\")\n# pred_ANN = model.predict(x_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 5. Logistic Regression","metadata":{}},{"cell_type":"code","source":"model_lr = LogisticRegression(n_jobs = -1, max_iter=1000)\nstart = time()\nmodel_lr.fit(x_train, y_train)\nend = time()\nprint(\"--------------------------------------------------------\")\nprint(f\"Time taken: {end - start} seconds\")\nprint(\"--------------------------------------------------------\")\npred_lr = model_lr.predict(x_test)\nacc = accuracy_score(pred_lr , y_test)\ncm = confusion_matrix(pred_lr , y_test)\ncr = classification_report(pred_lr , y_test)\nprint(acc)\nprint(\"--------------------------------------------------------\")\nprint(cm)\nprint(\"--------------------------------------------------------\")\nprint(cr)\nprint(\"--------------------------------------------------------\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 6. Decision Trees","metadata":{}},{"cell_type":"code","source":"model_DT = DecisionTreeClassifier()\nstart = time()\nmodel_DT.fit(x_train, y_train)\nend = time()\nprint(\"--------------------------------------------------------\")\nprint(f\"Time taken: {end - start} seconds\")\nprint(\"--------------------------------------------------------\")\npred_DT = model_DT.predict(x_test)\nacc = accuracy_score(pred_DT , y_test)\ncm = confusion_matrix(pred_DT , y_test)\ncr = classification_report(pred_DT , y_test)\nprint(acc)\nprint(\"--------------------------------------------------------\")\nprint(cm)\nprint(\"--------------------------------------------------------\")\nprint(cr)\nprint(\"--------------------------------------------------------\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 7. K-Nearest Neighbours","metadata":{}},{"cell_type":"code","source":"model_KN = KNeighborsClassifier()\nstart = time()\nmodel_KN.fit(x_train, y_train)\nend = time()\nprint(\"--------------------------------------------------------\")\nprint(f\"Time taken: {end - start} seconds\")\nprint(\"--------------------------------------------------------\")\npred_KN = model_KN.predict(x_test)\nacc = accuracy_score(pred_KN , y_test)\ncm = confusion_matrix(pred_KN , y_test)\ncr = classification_report(pred_KN , y_test)\nprint(acc)\nprint(\"--------------------------------------------------------\")\nprint(cm)\nprint(\"--------------------------------------------------------\")\nprint(cr)\nprint(\"--------------------------------------------------------\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 8. MLP Classifier","metadata":{}},{"cell_type":"code","source":"model_NN = MLPClassifier()\nstart = time()\nmodel_NN.fit(x_train, y_train)\nend = time()\nprint(\"--------------------------------------------------------\")\nprint(f\"Time taken: {end - start} seconds\")\nprint(\"--------------------------------------------------------\")\npred_NN = model_NN.predict(x_test)\nacc = accuracy_score(pred_NN , y_test)\ncm = confusion_matrix(pred_NN , y_test)\ncr = classification_report(pred_NN , y_test)\nprint(acc)\nprint(\"--------------------------------------------------------\")\nprint(cm)\nprint(\"--------------------------------------------------------\")\nprint(cr)\nprint(\"--------------------------------------------------------\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 9. XGBoost","metadata":{}},{"cell_type":"code","source":"model_XG = xgb.XGBClassifier()\nstart = time()\nmodel_XG.fit(x_train,y_train)\nend = time()\nprint(\"--------------------------------------------------------\")\nprint(f\"Time taken: {end - start} seconds\")\nprint(\"--------------------------------------------------------\")\npred_XG = model_XG.predict(x_test)\nacc = accuracy_score(pred_XG , y_test)\ncm = confusion_matrix(pred_XG , y_test)\ncr = classification_report(pred_XG , y_test)\nprint(acc)\nprint(\"--------------------------------------------------------\")\nprint(cm)\nprint(\"--------------------------------------------------------\")\nprint(cr)\nprint(\"--------------------------------------------------------\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 10. AdaBoost","metadata":{}},{"cell_type":"code","source":"model_ada = AdaBoostClassifier()\nstart = time()\nmodel_ada.fit(x_train,y_train)\nend = time()\nprint(\"--------------------------------------------------------\")\nprint(f\"Time taken: {end - start} seconds\")\nprint(\"--------------------------------------------------------\")\npred_ada = model_ada.predict(x_test)\nacc = accuracy_score(pred_ada , y_test)\ncm = confusion_matrix(pred_ada , y_test)\ncr = classification_report(pred_ada , y_test)\nprint(acc)\nprint(\"--------------------------------------------------------\")\nprint(cm)\nprint(\"--------------------------------------------------------\")\nprint(cr)\nprint(\"--------------------------------------------------------\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 12. Gaussian Naive Bayes","metadata":{}},{"cell_type":"code","source":"model_gnb = GaussianNB()\nstart = time()\nmodel_gnb.fit(x_train,y_train)\nend = time()\nprint(\"--------------------------------------------------------\")\nprint(f\"Time taken: {end - start} seconds\")\nprint(\"--------------------------------------------------------\")\npred_gnb = model_gnb.predict(x_test)\nacc = accuracy_score(pred_gnb , y_test)\ncm = confusion_matrix(pred_gnb , y_test)\ncr = classification_report(pred_gnb , y_test)\nprint(acc)\nprint(\"--------------------------------------------------------\")\nprint(cm)\nprint(\"--------------------------------------------------------\")\nprint(cr)\nprint(\"--------------------------------------------------------\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}